# Face-Arm: AI 기반 협동 로봇 작업 어시스턴트

### 📅 프로젝트 개요

* **주제**: Face-Arm (서비스 협동 로봇)
* **목표**: 고령화 사회의 인력 부족 문제를 해결하기 위한 서비스 로봇 구현
* **팀원**: 4명

---

## 📌 목차

1. [개발 배경 및 기획](#1-개발-배경-및-기획)  
2. [주요 기능 구현 과정](#2-주요-기능-구현-과정)  
3. [도전 과제와 문제 해결 방법](#3-도전-과제와-문제-해결-방법)  
4. [협업 내용 (맡은 역할), 진행 과정](#4-협업-내용-맡은-역할-진행-과정)  
5. [성과 및 결과물](#5-성과-및-결과물)  
6. [프로젝트 결과 (목표와 성과)](#6-프로젝트-결과-목표와-성과)  
7. [프로젝트 화면 & 기능 데모](#7-프로젝트-화면--기능-데모)  
8. [후기 및 향후 개선 사항](#8-후기-및-향후-개선-사항)  
9. [개인적 성찰 & 배운 점](#9-개인적-성찰--배운-점)  
10. [개선 및 확장 아이디어](#10-개선-및-확장-아이디어)

---

## 1. 개발 배경 및 기획

현재 한국은 고령사회에 진입했으며 초고령화 사회로의 전환을 앞두고 있습니다. 이에 따라 외국인 간병인 도입, 국민연금제도 개혁 등 다양한 노인복지 서비스와 정책이 제시되고 있지만, 여전히 뚜렷한 해결책을 찾지 못하고 있는 상황입니다. 이러한 문제는 사회 전반의 노인복지 부담을 가중시키는 요인으로 작용합니다.

본 프로젝트는 이러한 사회적 문제를 기술적으로 해결하기 위한 시도로, **노인복지 인력 및 인건비 부담을 줄이기 위한 서비스 협동 로봇**을 개발하고자 기획되었습니다.

### 핵심 기능

1. 음성 명령 인식 및 로봇 동작 트리거
2. 사용자 및 물체의 위치 인식
3. 로봇이 물체를 집어 사용자에게 전달

---

## 2. 주요 기능 구현 과정

### 2-1. 🎤 음성 명령 인식 (STT 기반 로봇 제어 트리거)

* **기술 스택 및 구조**

  * STT 엔진: OpenAI API
  * Wake-up Word 엔진: Whisper
  * 기타: 프롬프트 규칙 설계, 리소스 최적화

* **구현 내용**

  * Wake-up Word → STT → 명령어 추출 구조로 구현
  * 초기에는 STT가 상시 대기하는 구조였지만, 리소스 효율성 향상을 위해 wake-up 키워드(“Hello Rokey”) 인식 후 STT가 실행되도록 개선
  * 명령어 분석 후 ‘이동’, ‘물체 집기’, ‘사용자에게 전달’, ‘초기화’ 등 다양한 로봇 행동으로 연결

---

### 2-2. 📷 사용자 및 물체 위치 인식

* **기술 스택 및 구조**

  * 사용자 감지 카메라: Logitech C270 (RGB)
  * 물체 인식 카메라: RealSense D435i (RGB-D)
  * 객체 인식: YOLO (ultralytics)
  * 캘리브레이션: OpenCV `cv2.calibrateHandEye()`
  * 좌표 변환: numpy, Hand-Eye 결과 변환 행렬(T_gripper2camera.npy)

* **구현 내용**

  * RealSense D435i로 물체의 3D 위치(x, y, z) 추출 (YOLO + Depth)
  * Eye-in-Hand 방식으로 로봇팔에 카메라 부착됨, 따라서 외부 파라미터(Hand-Eye Calibration) 수행

    * Checkerboard를 다양한 각도에서 촬영 → `cv2.calibrateHandEye()` 사용  
    * T_gripper2camera.npy 생성 → 카메라 좌표 → 로봇 기준 좌표로 변환  
  * Logitech C270을 통해 얼굴 위치(x, y) 인식 (YOLO) 
  * 해당 픽셀 좌표를 로봇 작업 공간 좌표(x, z)로 선형 리매핑 

    * X: 50 ~ 660 mm, Z: 330 ~ 630 mm 등 실측 작업 범위를 하드코딩
    * 해상도/좌표계 축 방향 차이 및 실제 공간 제약을 고려하여 보정 수행

---

### 2-3. ⚙️ 로봇 동작 제어 및 물체 전달

* **기술 스택 및 구조**

  * ROS 2 서비스 통신 (`/get_3d_position`, `/get_keyword`)
  * 로봇 제어: DSR_ROBOT2 라이브러리 (`movej`, `movel`, `get_tool_force` 등)
  * 그리퍼 제어: OnRobot RG2 + 센서 상태 확인 (`get_status()` 사용)
  * 음성 안내: pydub (mp3 재생), Naver Clova TTS (음성 더빙)

* **구현 내용**

  * YOLO 기반 인식된 물체 좌표를 `/get_3d_position` 서비스로 받아 로봇 이동 좌표로 변환
  * Hand-Eye 캘리브레이션 결과(`T_gripper2camera.npy`)와 현재 로봇 위치(`get_current_posx()`)를 조합하여 카메라 좌표를 로봇 기준 좌표계로 변환<br>
  (YOLO 박스 중심 픽셀 좌표 → RealSense 카메라 좌표 → 로봇 기준 좌표로 변환)
  * 이동 및 Pick & Place는 `movej`, `movel`, RG2 그리퍼를 통해 수행
  * 그리퍼 센서 데이터 기반 `check_grip()` 사용자 정의 함수 구현
    * `OnRobot.get_status()`, `DSR_ROBOT2.get_tool_force()` 활용 
    * 센서 데이터를 기반으로 물체 접촉 여부 판단
    * 외력 변화가 감지 시 그리퍼 동작(열기/닫기) + 음성(mp3) 안내 수행

---

## 3. 도전 과제와 문제 해결 방법

  * STT 리소스 낭비 문제를 Wake-up 구조로 해결
  * Logitech C270과 로봇 좌표계의 축/단위 차이 문제 → 실측 기반 리매핑 및 보정
  * Hand-Eye Calibration 과정에서 `method` 값을 바꿔가며 반복 측정하여 오차 최소화
    * 그럼에도 불구하고 오차가 존재하여 하드코딩한 offset 보정값을 적용해 최종 조정

---

## 4. 협업 내용 (맡은 역할), 진행 과정
* **협업 내용**
각 팀원은 주어진 파트를 주도적으로 맡아 개발했지만, 모든 기능은 유기적으로 연결되어 있어 팀원 간의 긴밀한 협력이 필수적이었습니다. 예를 들어, 위치 인식을 담당한 팀원은 음성 인식 시스템과의 통합을 고려해 데이터 흐름을 조율했고, 저는 음성 인식 파트를 담당하며 다른 기능과 원활하게 연동될 수 있도록 STT 트리거 설계를 진행했습니다. 또한, 이해도 차이가 있는 부분에 대해선 문서화 및 공유를 통해 팀 전체의 역량을 끌어올렸습니다.

* **진행 과정**
  * 음성 인식 및 STT 트리거 파트를 담당
  * Wake-up 구조로 전환하여 성능 최적화
  * 위치 인식, 로봇 제어 파트 팀원과 긴밀히 연동하여 통합 개발
  * 기술 이해도 차이를 줄이기 위해 문서 공유 및 피드백 주도

---

## 5. 성과 및 결과물

  * 실시간 음성 명령으로 물체를 집어 사용자 위치로 전달하는 전 과정을 구현
  * ROS2 기반의 전체 연동 구조 구축
  * 협동 로봇의 다양한 센서 및 인공지능 모듈 통합 경험

## 6. 프로젝트 결과 (목표와 성과)

  * 기획한 3가지 핵심 기능 모두 구현
  * 음성 기반 제어, 객체 인식, 로봇 제어 모듈 통합 성공
  * 사용자 피드백 기반으로 로봇이 물체를 정확히 전달하는 시연 완료

## 7. 프로젝트 화면 & 기능 데모

![image](https://github.com/user-attachments/assets/2b0b3f95-ca7c-4aef-9a30-81f8efa3dd3e)

## 8. 후기 및 향후 개선 사항

  * STT의 한글 인식률 향상을 위한 추가 학습 데이터 필요
  * 사람이 특이점 위치에 놓일 경우 대응 범위 확대 필요
  * 다양한 명령어 대응 및 음성 피드백 확장 고려

## 9. 개인적 성찰 & 배운 점

  * 단일 파트가 아니라 시스템 전체의 연동과 리소스 관점에서 설계하는 사고 습득
  * 협업에서 문서화와 소통의 중요성을 체감
  * 실제 하드웨어 기반 프로젝트에서 발생하는 다양한 예외를 경험하며 문제 해결 역량 향상

## 10. 개선 및 확장 아이디어

> (작성 예정)

