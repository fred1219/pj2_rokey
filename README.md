# pj2_rokey

협동-2: AI(Computer Vision)기반 협동 로봇 작업 어시스턴트 구현 프로젝트<br>
주제: Face-Arm(서비스 협동 로봇)

1. 개발 과정<br>
현재 한국은 고령사회에 진입했으며 초고령화사회로의 전환을 앞두고 있습니다. 따라서 외국인 간병인 도입, 국민연금제도 개혁 등 노인복지에 대한 서비스와 정책들이 나오고 있지만 아직 갈피를 못찾고 있는 상황이며 이는 곧 노인복지부담을 증가시키는 요소 중 하나라고 생각합니다.
이러한 이유로 노인복지에 필요한 인력과 그에 따른 인건비 문제를 서비스 협동 로봇으로 대체함으로써 노인복지부담을 해소시키고자 서비스 협동 로봇을 주제로 기획하였습니다.
프로젝트 팀원은 총 4명이였으며 구현하고자 하는 프로젝트 핵심 기능은 '음성 명령(로봇 동작 트리거)', '사용자 & 물체 위치 인지', '로봇이 물체를 집어 사용자에게 전달' 3가지로 여기에 예외처리 사항을 추가하여 담당 파트를 계획 및 분배하였습니다.
이 중 음성 명령(로봇 동작 트리거) 파트를 담당하여 키워드 추출 프롬프트 규칙을 학습시키며 그에 따른 동작 트리거 방식을 구현하였으며 리소스 관점을 생각하여 stt가 상시 실행되는 토픽 구조에서 wake_up 키워드를 통해 요청이 되었을때 stt가 실행이 되도록 서비스 구조로 변경을 하여 리소스 부분을 줄였습니다.
이러한 과정에서 팀원 각자의 개발에 대한 이해도가 다르기때문에 모르는 부분에 대해서는 적극적인 표현을 통해 함께 지식을 공유하기도 하였으며 또한 이해도 향상에 도움을 주기 위해 기여했습니다.
---
현재 한국은 고령사회에 진입했으며, 초고령화 사회로의 전환을 앞두고 있습니다. 이에 따라 외국인 간병인 도입, 국민연금제도 개혁 등 다양한 노인복지 서비스와 정책이 제시되고 있으나, 여전히 뚜렷한 해결책을 찾지 못하고 있는 상황입니다. 이러한 문제는 곧 사회 전반의 노인복지 부담을 가중시키는 요인으로 작용하고 있습니다.<br>
본 프로젝트는 이러한 사회적 문제를 기술적으로 해결하기 위한 시도로, 노인복지 인력 및 인건비 부담을 줄이기 위한 서비스 협동 로봇 개발을 목표로 기획되었습니다.<br>
팀원은 총 4명으로 구성되었으며, 프로젝트의 핵심 기능은 다음 세 가지입니다.
  1. 음성 명령 인식 및 로봇 동작 트리거
  2. 사용자 및 물체의 위치 인식
  3. 로봇이 물체를 집어 사용자에게 전달<br>

저는 이 중 음성 명령 인식 파트를 담당하였으며, 사용자의 음성에서 특정 키워드를 추출하여 로봇의 동작을 유도하는 트리거 방식을 구현했습니다. 처음에는 STT(Speech-to-Text)가 상시 실행되는 구조였으나, 리소스 효율성을 고려하여 wake_up 키워드 인식 후에 STT가 실행되도록 서비스 구조를 변경하였습니다. 
이를 통해 불필요한 리소스 낭비를 줄이고 시스템의 안정성을 향상시켰습니다. 
프로젝트 전반에서 각자 맡은 역할에만 몰두하지 않고, 팀원들과 긴밀하게 협력하며 프로젝트의 성공적인 완수를 위해 함께 노력했습니다. 각 팀원은 자신의 파트를 주도적으로 수행하면서도, 필요한 부분에서 서로를 지원하고 협업하며, 기술적인 도전과제를 함께 해결해 나갔습니다. 또한, 개발 이해도 차이를 극복하기 위해 서로 적극적으로 소통하며 지식을 공유했고, 이해도가 낮은 부분에 대해서는 문서화 및 설명을 통해 협업의 질을 높였습니다.

2. 주요 기능 구현 과정

2-1. 음성 명령 인식 (로봇 동작 트리거)
기술 스택 및 구조

STT 엔진: OpenAI API

Wake-up Word 엔진: Whisper

기타 기술: 프롬프트 규칙 설계, 리소스 최적화

구현 내용

Wake-up Word → STT → 명령어 추출 구조로 구현

초기에는 STT가 상시 대기하는 구조였으나, 리소스 효율성 향상을 위해 wake-up 키워드("Hello Rokey") 인식 후 STT가 실행되는 구조로 개선

명령어 분석 후 ‘이동’, ‘물체 집기’, ‘사용자에게 전달’, ‘초기화’ 등 다양한 로봇 행동으로 연결

2-2. 사용자 및 물체 위치 인식
기술 스택 및 구조

카메라: RealSense D435i (RGB-D), Logitech C270 (RGB)

객체 인식: YOLO 모델 (ultralytics YOLO)

캘리브레이션: OpenCV cv2.calibrateHandEye()

좌표 변환 및 리매핑: numpy, hand-eye calibration 결과 파일 (T_gripper2camera.npy)

구현 내용

RealSense D435i 카메라로 물체 감지 및 3D 위치(x, y, z) 추출 → YOLO 객체 인식과 Depth 정보 기반

카메라를 로봇팔에 부착(Eye-in-Hand 구성)하여 외부 파라미터(Hand-Eye Calibration) 수행
→ Checkerboard를 다양한 각도에서 촬영한 이미지로 cv2.calibrateHandEye() 사용
→ 생성된 변환 행렬(T_gripper2camera.npy)을 통해 카메라 좌표 → 로봇 기준 좌표 변환

Logitech C270 RGB 카메라로 사용자의 얼굴 위치(x, y) 인식

카메라 화면 좌표계를 협동로봇 기준 좌표계(x, z)로 리매핑
→ 실제 작업 공간을 측정해 하드코딩한 값(예: X: 50660 mm, Z: 330630 mm) 사용
→ 해상도 차이, 좌표계 축 방향 차이, 실세계 물리적 공간 제약을 반영하여 보정

2-3. 로봇 동작 제어 및 물체 전달
기술 스택 및 구조

ROS2 기반 서비스 통신 (서비스 /get_3d_position, /get_keyword)

로봇 제어 명령: DSR_ROBOT2 라이브러리 (movej, movel, get_tool_force 등)

그리퍼 제어: OnRobot RG2 라이브러리

음성 안내: pydub 라이브러리 (mp3 재생), Naver Clova (음성 더빙)

구현 내용

YOLO 기반 인식된 물체 좌표를 /get_3d_position 서비스로 받아 로봇 이동 좌표로 변환

Hand-Eye 캘리브레이션 결과(T_gripper2camera.npy)와 현재 로봇 위치(get_current_posx())를 조합하여 카메라 좌표를 로봇 기준 좌표계로 변환

(YOLO 박스 중심 픽셀 좌표 → RealSense 카메라 좌표 → 로봇 좌표 변환)

이동 및 Pick & Place는 movej, movel, RG2 그리퍼를 활용해 수행

그리퍼 센서 데이터를 활용한 직접 구현 함수 check_grip() 사용 (OnRobot.get_status()와 DSR_ROBOT2.get_tool_force() 활용)
→ 센서 데이터를 기반으로 물체 접촉 여부 판단
→ 외력 변화가 감지되면 그리퍼를 자동으로 열거나 닫으며, 음성 안내(mp3)를 통해 동작 상태를 사용자에게 알림

3. 도전 과제와 문제 해결 방법

4. 협업 내용 (맡은 역할), 진행 과정
팀원 각자는 각자의 담당 파트를 맡아 주도적인 역할을 했지만, 각 기능이 서로 유기적으로 연결되었기 때문에 우리는 상호 협력을 통해 기능을 통합했습니다. 예를 들어, 위치 인식을 담당한 팀원은 저와 함께 사람 감지 카메라의 데이터를 정확히 로봇의 동작에 반영할 수 있도록 STT 시스템과의 통합을 적극적으로 도왔습니다.
제가 맡은 음성 명령 처리 부분에서 얻은 경험을 다른 팀원들과 공유하며, 기술적 문제를 빠르게 해결할 수 있었습니다. 또한, 각자의 전문성을 살려 다양한 문제를 공동으로 해결한 덕분에 프로젝트 전반에 걸쳐 효율적인 협업을 이룰 수 있었습니다.
5. 성과 및 결과물

6. 프로젝트 결과 (목표와 성과)

7. 프로젝트 화면 및 기능 데모
![image](https://github.com/user-attachments/assets/2b0b3f95-ca7c-4aef-9a30-81f8efa3dd3e)

8. 후기 및 향후 개선 사항

9. 개인적 성찰 및 배운 점

10. 개선 및 확장 아이디어
