# Face-Arm: AI 기반 협동 로봇 작업 어시스턴트

### 📅 프로젝트 개요

* **주제**: Face-Arm (서비스 협동 로봇)
* **목표**: 고령화 사회의 인력 부족 문제를 해결하기 위한 서비스 로봇 구현
* **팀원**: 4명

---

## 📌 목차

1. [개발 배경 및 기획](#1-개발-배경-및-기획)
2. [사용 기술 스택](#2-사용-기술-스택)
3. [주요 기능 구현 과정](#3-주요-기능-구현-과정)
4. [도전 과제와 문제 해결 방법](#4-도전-과제와-문제-해결-방법)
5. [협업 내용 및 진행 과정](#5-협업-내용-및-진행-과정)
6. [성과 및 결과물](#6-성과-및-결과물)
7. [프로젝트 결과](#7-프로젝트-결과)
8. [기능 데모](#8-기능-데모)
9. [후기 및 향후 개선 사항](#9-후기-및-향후-개선-사항)
10. [개인적 성찰 및 배운 점](#10-개인적-성찰-및-배운-점)
11. [개선 및 확장 아이디어](#11-개선-및-확장-아이디어)

---

## 1. 개발 배경 및 기획

현재 한국은 고령사회에 진입했으며 초고령화 사회로의 전환을 앞두고 있습니다. 이에 따라 외국인 간병인 도입, 국민연금제도 개혁 등 다양한 노인복지 서비스와 정책이 제시되고 있지만, 여전히 뚜렷한 해결책을 찾지 못하고 있는 상황입니다. 이러한 문제는 사회 전반의 노인복지 부담을 가중시키는 요인으로 작용합니다.

**Face-Arm 프로젝트**는 이러한 사회적 문제를 기술적으로 해결하고자, **노인복지 인력 및 인건비 부담을 줄이기 위한 서비스 협동 로봇** 개발을 목표로 기획되었습니다.

### 핵심 기능

1. 음성 명령 인식 및 로봇 동작 트리거
2. 사용자 및 물체의 위치 인식
3. 로봇이 물체를 집어 사용자에게 전달

---

## 2. 사용 기술 스택

| 분류       | 기술                                             |
| -------- | ---------------------------------------------- |
| 언어       | Python                                         |
| 로봇 제어    | ROS 2 (Humble), Doosan DSR\_ROBOT2 API           |
| 음성 인식    | OpenAI API (STT), Whisper (Wake-up Word)       |
| 음성 출력    | pydub (mp3 재생), Naver Clova TTS (음성 더빙)    |
| 객체 인식    | YOLOv8 (Ultralytics)                           |
| Depth 센서 | Intel RealSense D435i                          |
| RGB 카메라  | Logitech C270                                  |
| 그리퍼 제어   | OnRobot RG2                                    |
| 캘리브레이션   | OpenCV `cv2.calibrateHandEye()`                |
| 통신 구조    | ROS 2 토픽 (`/remapped_coord`)<br> ROS 2 서비스 (`/get_3d_position`, `/get_keyword`)  |

---

## 3. 주요 기능 구현 과정

### 3-1. 🎤 음성 명령 인식 (STT 기반 제어 트리거)

* **기술 스택 및 구조**

  * **STT 엔진**: OpenAI API
  * **Wake-up Word 엔진**: Whisper
  * **기타**: 프롬프트 규칙 설계, 리소스 최적화

* **구현 내용**

  * Wake-up Word → STT → 명령어 추출 구조로 구현
  * 초기에는 STT가 상시 대기하는 구조였지만, 리소스 효율성 향상을 위해 wake-up 키워드(“Hello Rokey”) 인식 후 STT가 실행되도록 개선
  * 명령어 분석 후 ‘이동’, ‘물체 집기’, ‘사용자에게 전달’, ‘초기화’ 등 다양한 로봇 행동으로 연결

---

### 3-2. 📷 사용자 및 물체 위치 인식

* **기술 스택 및 구조**

  * **사용자 감지 카메라**: Logitech C270 (RGB)
  * **물체 인식 카메라**: RealSense D435i (RGB-D)
  * **객체 인식**: YOLO (ultralytics)
  * **캘리브레이션**: OpenCV `cv2.calibrateHandEye()`
  * **좌표 변환**: numpy, Hand-Eye 결과 변환 행렬(T_gripper2camera.npy)

* **구현 내용**
  * RealSense D435i: Depth 기반으로 YOLO가 검출한 물체의 3D 위치(x, y, z) 추출
  * Logitech C270: 사용자의 얼굴을 YOLO로 인식 → (x, y) 화면 좌표
  * Hand-Eye Calibration을 통해 Eye-in-Hand 구조에서의 외부 파라미터 구함
  
    * Checkerboard를 다양한 각도에서 촬영 → `cv2.calibrateHandEye()` 사용
    * 생성된 행렬(T_gripper2camera.npy)로 로봇 기준 좌표 변환
  * 사용자의 얼굴 좌표계(x, y)를 실제 로봇 좌표계(x, z)로 선형 리매핑
  
    * X: 50 ~ 660 mm, Z: 330 ~ 630 mm 실측 작업 범위를 하드코딩
    * 해상도/좌표계 축 방향 차이 및 실제 공간 제약을 고려하여 보정

---

### 3-3. ⚙️ 로봇 동작 제어 및 물체 전달

* **기술 스택 및 구조**

  * **로봇 제어**: DSR_ROBOT2 라이브러리 (`movej`, `movel`, `get_tool_force` 등)
  * **그리퍼 제어**: OnRobot RG2 + 센서 상태 확인 (`get_status()` 사용)
  * **음성 안내**: pydub (mp3 재생), Naver Clova TTS (음성 더빙)
    
* **구현 내용**

    * YOLO 기반 인식된 물체 좌표를 `/get_3d_position` 서비스로 받아 로봇 이동 좌표로 변환
    * Hand-Eye 결과(`T_gripper2camera.npy`)와 현재 로봇 위치(`get_current_posx()`)를 조합하여 로봇 기준 좌표계로 변환<br>
    (YOLO 박스 중심 픽셀 좌표 → RealSense 카메라 좌표 → 로봇 기준 좌표로 변환)
    * 이동 및 Pick & Place는 `movej`, `movel`, RG2 그리퍼를 통해 수행
    * 그리퍼 센서 데이터 기반 `check_grip()` 사용자 정의 함수 구현
      * `OnRobot.get_status()`, `DSR_ROBOT2.get_tool_force()` 활용 
      * 센서 데이터를 기반으로 물체 접촉 여부 판단
      * 외력 변화가 감지 시 그리퍼 동작(열기/닫기) + 음성(mp3) 안내 수행

---

## 4. 도전 과제와 문제 해결 방법

* **STT 리소스 낭비 문제** → wake-up 구조로 전환해 해결
* **Logitech C270 좌표계 오차** → 실측 기반 리매핑 및 선형 보정
* **Hand-Eye Calibration 오차** → 다양한 method 값으로 반복 실험, offset 보정 적용
* **로봇 접근 제한 구간 이슈** → 제한 영역 사전 설정 및 예외처리로 안정성 확보

---

## 5. 협업 내용 및 진행 과정

* **협업 내용**
각 팀원은 주어진 파트를 주도적으로 맡아 개발했지만, 모든 기능은 유기적으로 연결되어 있어 팀원 간의 긴밀한 협력이 필수적이었습니다. 예를 들어, 위치 인식을 담당한 팀원은 음성 인식 시스템과의 통합을 고려해 데이터 흐름을 조율했고, 저는 음성 인식 파트를 담당하며 다른 기능과 원활하게 연동될 수 있도록 STT 트리거 설계를 진행했습니다. 또한, 이해도 차이가 있는 부분에 대해선 문서화 및 공유를 통해 팀 전체의 역량을 끌어올렸습니다.

* **진행 과정**
  * 음성 인식 및 STT 트리거 파트를 담당
  * Wake-up 구조로 전환하여 성능 최적화
  * 위치 인식, 로봇 제어 파트 팀원과 긴밀히 연동하여 통합 개발
  * 기술 이해도 차이를 줄이기 위해 문서 공유 및 피드백 주도
---

## 6. 성과 및 결과물

* 실시간 음성 명령 기반 로봇 제어 시스템 구현
* 물체 인식 → 집기 → 사용자 위치 전달까지 자동화
* ROS 2 기반 분산 구조로 시스템 구성
* 실제 협동 로봇, Depth 센서, RGB 카메라 등 다양한 센서 및 인공지능 모듈 통합 경험

---

## 7. 프로젝트 결과

* 목표한 3가지 핵심 기능 모두 구현
* 사용자 피드백 기반으로 로봇이 물체를 정확히 전달하는 시연 완료
* HRI(Human-Robot Interaction)를 중심으로 실환경 서비스 로봇 구조 완성
* 향후 다양한 복지/서비스 분야에 확장 가능한 프로토타입 확보


---

## 8. 기능 데모

[▶️ 영상 보기 (예시)](https://youtu.be/your_video_link)
또는
![Demo](https://github.com/user-attachments/assets/2b0b3f95-ca7c-4aef-9a30-81f8efa3dd3e)

---

## 9. 후기 및 향후 개선 사항

* STT 한글 인식률 추가 개선 필요
* 사용자가 특이 위치에 있을 때 좌표 계산 안정성 향상 필요
* 다양한 명령어 확장
* 음성 피드백 다양화 및 정서적 반응 기능 고려 가능

---

## 10. 개인적 성찰 및 배운 점

* 단일 파트가 아니라 시스템 전체의 연동과 리소스 관점에서 설계하는 사고 습득
* 실제 하드웨어 기반 프로젝트에서 발생하는 다양한 예외를 경험하며 문제 해결 역량 향상
* 협업 시 문서화와 공유의 중요성을 체감하며 소통 방식 개선

---

## 11. 개선 및 확장 아이디어

* **AMR + 협동로봇 결합 (모바일 매니퓰레이터)**: 현재는 로봇이 고정되어 있지만 AMR을 활용하면 자유 이동 기반의 서비스 가능
* **실제 복지 시설과 연계**: 예: 노블카운티 요양원 등과 연계하여 필드 테스트 및 피드백 적용
* **표정 인식 기반 반응**: 단순 위치 인식 외에도 사용자의 감정 상태를 파악해 반응하는 정서 지능 기능 추가 고려

---
