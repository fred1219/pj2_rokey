# Face-Arm: AI 기반 협동 로봇 작업 어시스턴트

## 📑 프로젝트 개요

- **주제**: Face-Arm (서비스 협동 로봇)
- **목표**: 고령화 사회의 인력 부족 문제를 해결하기 위한 서비스 로봇 구현
- **팀원**: 4명

**👥 팀원 역할 분담**
>|이름       |	역할      |
>|-----------|------------|
>|최정호 (팀장)|	객체 인식 모델 학습 및 발표|
>|이하빈|	얼굴 인식 모델 학습 및 좌표 리매핑|
>|이세현|	프롬프트 코딩 및 로봇 동작부 코딩|
>|홍진규|	예외처리 코드 작성|
<br>

**📅 작업 일정**
> |기간	| 작업 내용 |
> |-----------|------------|
> |5/23 ~ 5/26 |	프로젝트 기획 및 주제 선정|
> |5/27	| 서비스 로봇 관련 자료 조사|
> |5/28 ~ 5/29 |	파트별 코드 개발 (초기 버전)|
> |5/30	|중간 점검 및 코드 통합 테스트|
> |6/1 ~ 6/3	|최적화 및 오류 수정|
> |6/4 ~ 6/5	| 최종 테스트 및 시연 준비|
> |총 개발 기간 |	2025.05.23 ~ 06.05 (10일)|
---

## 📌 목차

1. [개발 배경 및 기획](#1-개발-배경-및-기획)  
2. [사용 기술 스택](#2-사용-기술-스택)  
3. [주요 기능 구현 과정](#3-주요-기능-구현-과정)  
4. [핵심 코드 구현](#4-핵심-코드-구현)  
5. [도전 과제와 문제 해결 방법](#5-도전-과제와-문제-해결-방법)  
6. [협업 내용 및 진행 과정](#6-협업-내용-및-진행-과정)  
7. [성과 및 결과물](#7-성과-및-결과물)  
8. [프로젝트 결과](#8-프로젝트-결과)  
9. [기능 데모](#9-기능-데모)  
10. [후기 및 향후 개선 사항](#10-후기-및-향후-개선-사항)  
11. [개인적 성찰 및 배운 점](#11-개인적-성찰-및-배운-점)  
12. [개선 및 확장 아이디어](#12-개선-및-확장-아이디어)  

---

## 1. 개발 배경 및 기획

한국은 초고령화 사회로 진입하고 있으며, 간병 및 복지 분야의 인력 수요가 빠르게 증가하고 있습니다. 이에 따른 비용 및 노동 부담을 완화하고자, AI 및 협동로봇을 결합한 자동화 서비스의 가능성을 실험했습니다.

**Face-Arm 프로젝트**는 음성 명령, 객체 인식, 사람 위치 인식, 로봇 제어 기능을 융합해 **서비스형 협동 로봇 프로토타입**을 구현하는 것이 목표였습니다.

### 🔧 핵심 기능

1. 음성 명령 인식 및 제어 트리거  
2. 사용자(얼굴) 및 객체 위치 인식  
3. 물체 집기 및 사용자에게 전달

### 🧭 프로세스
![협동2 시나리오](https://github.com/user-attachments/assets/8c660b3f-d159-4f77-a794-29fbb06aab93)


---

## 2. 사용 장비 & 기술 스택
![image](https://github.com/user-attachments/assets/b4e5d91f-5fc2-43c5-81a3-100f6bc34f51)

| 분류       | 기술                                                |
|------------|-----------------------------------------------------|
| 언어       | Python                                              |
| 로봇 제어  | ROS 2 (Humble), Doosan DSR_ROBOT2 API              |
| 음성 인식  | OpenAI Whisper (STT), Wake-up Word                |
| 음성 출력  | pydub (mp3), Naver Clova TTS                       |
| 객체 인식  | YOLOv8 (Ultralytics)                               |
| 센서       | Intel RealSense D435i (RGB-D), Logitech C270 (RGB) |
| 그리퍼 제어 | OnRobot RG2                                        |
| 캘리브레이션 | OpenCV Hand-Eye (`cv2.calibrateHandEye()`)        |
| ROS 통신   | 토픽(`/remapped_coord`) + 서비스(`/get_keyword`, `/get_3d_position`) |

---

## 3. 주요 기능 구현 과정

### 3-1. 🎤 음성 명령 인식 (STT 기반 제어 트리거)

#### 📌 기술 스택 및 구조
- **STT 엔진**: OpenAI API  
- **Wake-up Word 엔진**: Whisper  
- **기타**: 프롬프트 규칙 설계 및 리소스 최적화  

#### ⚙️ 구현 내용
- ‘Hello Rokey’ 등 **Wake-up 키워드 인식 후 STT 활성화** 구조로 리소스 낭비 최소화  
- STT로부터 음성 명령어를 추출하고, ‘**이동**’, ‘**물체 집기**’, ‘**사용자에게 전달**’, ‘**초기화**’ 등 로봇 행동으로 연결  

---

### 3-2. 📷 사용자 및 물체 위치 인식

#### 📌 기술 스택 및 구조
- **사용자 감지**: Logitech C270 (RGB 카메라)  
- **물체 인식**: Intel RealSense D435i (RGB-D 카메라) + YOLOv8 (Ultralytics)  
- **캘리브레이션**: OpenCV `cv2.calibrateHandEye()`  
- **좌표 변환**: numpy, Hand-Eye 보정 행렬 (`T_gripper2camera.npy`)  

#### ⚙️ 구현 내용
- YOLOv8로 사용자 얼굴 및 물체 검출 후, **RealSense Depth 센서**로 물체 3D 좌표 추출  
- **Hand-Eye Calibration**을 통해 Eye-in-Hand 구조의 로봇과 카메라 좌표계 간 변환 행렬 확보  
- 얼굴 픽셀 좌표(x, y)를 로봇 기준 좌표(x, z)로 선형 리매핑하여 실제 작업 공간과 매칭  

![image](https://github.com/user-attachments/assets/568b801b-60e5-4e03-9aae-fa0ce0179a04)
<br>📷 _좌표 변환 시각화 이미지_

---

### 3-3. ⚙️ 로봇 동작 제어 및 물체 전달

#### 📌 기술 스택 및 구조
- **로봇 제어**: DSR_ROBOT2 라이브러리 (`movej`, `movel`, `get_tool_force` 등)  
- **그리퍼 제어**: OnRobot RG2 (센서 상태 확인 및 제어)  
- **음성 안내**: pydub (mp3 재생), Naver Clova TTS  

#### ⚙️ 구현 내용
- `/get_3d_position` 서비스로 YOLO 기반 인식 좌표 획득  
- Hand-Eye 보정 행렬과 로봇 현재 위치를 조합해 로봇 좌표계로 변환  
- 변환된 좌표를 활용해 **로봇 이동 및 그리퍼 동작 수행 (픽 앤 플레이스)**  
- **그리퍼 센서 상태 및 외력 변화 감지 기능**을 통해 물체 접촉 유무 판단  
- 상황에 따른 **동작 수행 + 음성 안내(mp3 재생)** 실행  

![image](https://github.com/user-attachments/assets/9c621ae6-a68b-4e1a-9789-3f7ead718614)
<br>🎥 _Pick & Place 동작 과정_

---

### 3-4. 🛡 예외 처리 및 안정성 확보

#### 🧪 예외 처리 내용
- **ROS 2 서비스 호출 실패** 및 응답 지연에 대비한 타임아웃 처리  
- **그리퍼 센서 이상** 시 재시도 및 오류 알림 기능 구현  
- **음성 명령 인식 실패** 시 재시도 로직 및 대기 상태 유지  
- **로봇 동작 중 외력 이상 감지** 시 안전 정지 및 사용자 알림  

#### 🔐 안정성 강화
- 예외 상황에 따른 **사용자 알림 및 음성 피드백 제공**으로 신뢰도 및 사용자 경험 향상  
- **다중 센서 정보 융합** 및 상태 점검을 통해 오작동 최소화  

---

## 4. 핵심 코드 구현

### `face_yolo.py`  
**역할**: YOLO 기반 얼굴 탐지 및 월드 좌표계 변환  
**주요 기능**: 얼굴 중심 픽셀 좌표 추출 → Hand-Eye Calibration 결과 행렬로 리맵핑 → `/remapped_coord` 토픽 발행  

**ROS 2 구성**:
- **노드명**: `face_yolo_node` (얼굴 탐지 및 좌표 변환)
- **토픽 발행**: `/remapped_coord` (3D 얼굴 위치 좌표)

---

### `get_keyword.py`  
**역할**: STT + LangChain 기반 음성 명령 키워드 추출  
**주요 기능**: Wake-up Word 인식 후 STT 수행 → 자연어 명령에서 핵심 키워드 추출 → `/get_keyword` 서비스 서버로 제공  

**ROS 2 구성**:
- **노드명**: `keyword_extraction_node` (음성 키워드 추출)
- **서비스 서버1**: `/get_keyword` (키워드 반환 서비스)

---

### `object_detection.py`  
**역할**: YOLO 객체 인식 및 RealSense 깊이 데이터 결합 3D 위치 산출  
**주요 기능**: 객체 중심 좌표 및 Depth 정보를 사용해 월드 3차원 위치 계산 → `/get_3d_position` 서비스 서버 제공  

**ROS 2 구성**:
- **노드명**: `object_detection_node` (객체 인식 및 위치 산출)
- **서비스 서버2**: `/get_3d_position` (3D 위치 응답 서비스)

---

### `robot_control.py`  
**역할**: 키워드 기반 로봇 동작 실행 및 위치 정보 활용 정밀 제어  
**주요 기능**:  
- `/get_keyword` 서비스 클라이언트 호출로 음성 텍스트에서 추출된 키워드 획득
- `/get_3d_position` 서비스 클라이언트 호출로 물체 위치 획득  
- `/remapped_coord` 토픽 구독으로 얼굴 위치 실시간 수신  
- 그리퍼 제어 및 외력 감지 기반 예외처리 포함  

**ROS 2 구성**:
- **노드명**: `robot_control_node` (로봇 제어 및 상태 관리)
- **토픽 구독**: `/remapped_coord` (얼굴 위치)
- **서비스 클라이언트1**: `/get_keyword` (키워드 추출 요청)
- **서비스 클라이언트2**: `/get_3d_position` (객체 위치 요청)

---

## 5. 도전 과제와 문제 해결 방법

| 문제 | 해결 방안 |
|------|------------|
| STT 리소스 낭비 문제 |  Wake-up 키워드 인식 후 STT 활성화 구조로 전환 |
| Logitech C270 좌표계 오차 | 해상도 및 거리 실측 기반 리맵핑 |
| Hand-Eye Calibration 오차 | 다양한 method 값 반복 실험, offset 보정 적용 |
| 로봇 접근 제한 구간 문제 | 제한 영역 사전 설정 및 예외처리로 안정성 확보 |

---

## 6. 협업 내용 및 진행 과정
* **협업 내용**

  > 각 팀원은 주어진 파트를 주도적으로 맡아 개발했지만, 모든 기능은 유기적으로 연결되어 있어 팀원 간의 긴밀한 협력이 필수적이었습니다. 예를 들어, 위치 인식을 담당한 팀원은 음성 인식 시스템과의 통합을 고려해 데이터 흐름을 조율했고, 저는 음성 인식 파트를 담당하며 다른 기능과 원활하게 연동될 수 있도록 STT 트리거 설계를 진행했습니다. 또한, 이해도 차이가 있는 부분에 대해선 문서화 및 공유를 통해 함께 팀 전체의 역량을 끌어올렸습니다.

* **진행 과정**
  
  - 음성 인식 및 STT 트리거 파트를 담당
  - 위치 인식, 로봇 제어 파트 팀원과 연동하여 1차 통합 개발
  - 예외처리 파트 팀원과 연동하여 2차 통합 개발
  - Wake-up 구조로 전환하여 성능 최적화
  - 기술 이해도 차이를 줄이기 위해 문서 공유 및 피드백 주도

---

## 7. 성과 및 결과물

- ROS 2 기반 실시간 STT-기반 협동 로봇 제어 시스템 구축
- 사용자 얼굴 인식 → 물체 전달 → 음성 피드백까지 통합 동작
- Hand-Eye, 센서 기반 좌표 변환까지 포함한 전체 파이프라인 구현

---

## 8. 프로젝트 결과

- **3가지 핵심 기능 전부 구현 성공**
- **기능 시연 완수**: 사용자 피드백 기반 사용자에게 정확히 물체 전달 성공
- **실환경 서비스 로봇 프로토타입 완성**
- **실제 협동 로봇 + 센서 + 인공지능 통합 경험 확보**

---

## 9. 기능 데모

🎥 [▶️ 시연 영상 보기](https://youtu.be/GI565ckTv_A)  
📷 _시스템 구조 다이어그램, YOLO 인식 화면 등 이미지 삽입 권장_

---

## 10. 후기 및 향후 개선 사항

- 사용자와의 거리가 멀 경우 **STT 한글 인식률** 추가 개선 필요 
- **복잡한 환경에서의 좌표 안정성** 추가 보정 고려  
- **명령어 다양화 및 감정 피드백 시스템 확장 가능성 확인**

---

## 11. 개인적 성찰 및 배운 점

- 단일 파트가 아니라 시스템 전체의 연동과 리소스 관점에서 설계하는 사고 습득
- 협업 시 문서화와 공유의 중요성을 체감하며 소통 방식 개선
- 실제 하드웨어 기반 프로젝트에서 발생하는 다양한 예외를 경험하며 문제 해결 역량 향상

---

## 12. 개선 및 확장 아이디어

- **AMR 기반 모바일 매니퓰레이터 확장**: 현재는 로봇이 고정되어 있지만 AMR을 활용하면 자유 이동 기반의 서비스 가능
- **복지 시설과 실증 협력**: ex)노블카운티 요양원 등과 연계하여 필드 테스트 및 피드백 적용
- **표정 인식 등 정서적 반응 기능 추가**: 단순 위치 인식 외에도 사용자의 감정 상태를 파악해 반응하는 정서 지능 기능 추가 고려

