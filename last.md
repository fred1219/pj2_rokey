# Face-Arm: AI 기반 협동 로봇 작업 어시스턴트

## 📑 프로젝트 개요

- **주제**: Face-Arm (서비스 협동 로봇)
- **목표**: 고령화 사회의 인력 부족 문제를 해결하기 위한 서비스 로봇 구현
- **팀원**: 4명

**👥 팀원 역할 분담**
>|이름       |	역할      |
>|-----------|------------|
>|최정호 (팀장)|	객체 인식 모델 학습 및 발표|
>|이하빈|	얼굴 인식 모델 학습 및 좌표 리매핑|
>|이세현|	프롬프트 코딩 및 로봇 동작부 코딩|
>|홍진규|	예외처리 코드 작성|
<br>

**📅 작업 일정**
> |기간	| 작업 내용 |
> |-----------|------------|
> |5/23 ~ 5/26 |	프로젝트 기획 및 주제 선정|
> |5/27	| 서비스 로봇 관련 자료 조사|
> |5/28 ~ 5/29 |	파트별 코드 개발 (초기 버전)|
> |5/30	|중간 점검 및 코드 통합 테스트|
> |6/1 ~ 6/3	|최적화 및 오류 수정|
> |6/4 ~ 6/5	| 최종 테스트 및 시연 준비|
> |총 개발 기간 |	2025.05.23 ~ 06.05 (10일)|
---

## 📌 목차

1. [개발 배경 및 기획](#1-개발-배경-및-기획)  
2. [사용 기술 스택](#2-사용-기술-스택)  
3. [주요 기능 구현 과정](#3-주요-기능-구현-과정)  
4. [핵심 코드 구현](#4-핵심-코드-구현)  
5. [도전 과제와 문제 해결 방법](#5-도전-과제와-문제-해결-방법)  
6. [협업 내용 및 진행 과정](#6-협업-내용-및-진행-과정)  
7. [성과 및 결과물](#7-성과-및-결과물)  
8. [프로젝트 결과](#8-프로젝트-결과)  
9. [기능 데모](#9-기능-데모)  
10. [후기 및 향후 개선 사항](#10-후기-및-향후-개선-사항)  
11. [개인적 성찰 및 배운 점](#11-개인적-성찰-및-배운-점)  
12. [개선 및 확장 아이디어](#12-개선-및-확장-아이디어)  

---

## 1. 개발 배경 및 기획

한국은 초고령화 사회로 진입하고 있으며, 간병 및 복지 분야의 인력 수요가 빠르게 증가하고 있습니다. 이에 따른 비용 및 노동 부담을 완화하고자, AI 및 협동로봇을 결합한 자동화 서비스의 가능성을 실험했습니다.

**Face-Arm 프로젝트**는 음성 명령, 객체 인식, 사람 위치 인식, 로봇 제어 기능을 융합해 **서비스형 협동 로봇 프로토타입**을 구현하는 것이 목표였습니다.

### 🔧 핵심 기능

1. 음성 명령 인식 및 제어 트리거  
2. 사용자(얼굴) 및 객체 위치 인식  
3. 물체 집기 및 사용자에게 전달

---

## 2. 사용 장비 & 기술 스택
![image](https://github.com/user-attachments/assets/b4e5d91f-5fc2-43c5-81a3-100f6bc34f51)

| 분류       | 기술                                                |
|------------|-----------------------------------------------------|
| 언어       | Python                                              |
| 로봇 제어  | ROS 2 (Humble), Doosan DSR_ROBOT2 API              |
| 음성 인식  | OpenAI Whisper (STT), Wake Word Trigger            |
| 음성 출력  | pydub (mp3), Naver Clova TTS                       |
| 객체 인식  | YOLOv8 (Ultralytics)                               |
| 센서       | Intel RealSense D435i (RGB-D), Logitech C270 (RGB) |
| 그리퍼 제어 | OnRobot RG2 + Force Sensor                        |
| 캘리브레이션 | OpenCV Hand-Eye (`cv2.calibrateHandEye()`)        |
| ROS 통신   | 토픽(`/remapped_coord`) + 서비스(`/get_3d_position`, `/get_keyword`) |

---

## 3. 주요 기능 구현 과정

### 3-1. 🎤 음성 명령 인식 (STT 기반 제어 트리거)

#### 📌 기술 스택 및 구조
- **STT 엔진**: OpenAI API  
- **Wake-up Word 엔진**: Whisper  
- **기타**: 프롬프트 규칙 설계 및 리소스 최적화  

#### ⚙️ 구현 내용
- ‘Hello Rokey’ 등 **Wake-up 키워드 인식 후 STT 활성화** 구조로 리소스 낭비 최소화  
- STT로부터 음성 명령어를 추출하고, ‘**이동**’, ‘**물체 집기**’, ‘**사용자에게 전달**’, ‘**초기화**’ 등 로봇 행동으로 연결  

---

### 3-2. 📷 사용자 및 물체 위치 인식

#### 📌 기술 스택 및 구조
- **사용자 감지**: Logitech C270 (RGB 카메라)  
- **물체 인식**: Intel RealSense D435i (RGB-D 카메라) + YOLOv8 (Ultralytics)  
- **캘리브레이션**: OpenCV `cv2.calibrateHandEye()`  
- **좌표 변환**: numpy, Hand-Eye 보정 행렬 (`T_gripper2camera.npy`)  

#### ⚙️ 구현 내용
- YOLOv8로 사용자 얼굴 및 물체 검출 후, **RealSense Depth 센서**로 물체 3D 좌표 추출  
- **Hand-Eye Calibration**을 통해 Eye-in-Hand 구조의 로봇과 카메라 좌표계 간 변환 행렬 확보  
- 얼굴 픽셀 좌표(x, y)를 로봇 기준 좌표(x, z)로 선형 리매핑하여 실제 작업 공간과 매칭  

![image](https://github.com/user-attachments/assets/568b801b-60e5-4e03-9aae-fa0ce0179a04)
<br>📷 _좌표 변환 시각화 이미지_

---

### 3-3. ⚙️ 로봇 동작 제어 및 물체 전달

#### 📌 기술 스택 및 구조
- **로봇 제어**: DSR_ROBOT2 라이브러리 (`movej`, `movel`, `get_tool_force` 등)  
- **그리퍼 제어**: OnRobot RG2 (센서 상태 확인 및 제어)  
- **음성 안내**: pydub (mp3 재생), Naver Clova TTS  

#### ⚙️ 구현 내용
- `/get_3d_position` 서비스로 YOLO 기반 인식 좌표 획득  
- Hand-Eye 보정 행렬과 로봇 현재 위치를 조합해 로봇 좌표계로 변환  
- 변환된 좌표를 활용해 **로봇 이동 및 그리퍼 동작 수행 (픽 앤 플레이스)**  
- **그리퍼 센서 상태 및 외력 변화 감지 기능**을 통해 물체 접촉 유무 판단  
- 상황에 따른 **동작 수행 + 음성 안내(mp3 재생)** 실행  

![image](https://github.com/user-attachments/assets/9c621ae6-a68b-4e1a-9789-3f7ead718614)
<br>🎥 _Pick & Place 동작 과정_

---

### 3-4. 🛡 예외 처리 및 안정성 확보

#### 🧪 예외 처리 내용
- **ROS 2 서비스 호출 실패** 및 응답 지연에 대비한 타임아웃 처리  
- **그리퍼 센서 이상** 시 재시도 및 오류 알림 기능 구현  
- **음성 명령 인식 실패** 시 재시도 로직 및 대기 상태 유지  
- **로봇 동작 중 외력 이상 감지** 시 안전 정지 및 사용자 알림  

#### 🔐 안정성 강화
- 예외 상황에 따른 **사용자 알림 및 음성 피드백 제공**으로 신뢰도 및 사용자 경험 향상  
- **다중 센서 정보 융합** 및 상태 점검을 통해 오작동 최소화  

---

## 4. 핵심 코드 구현

### `face_yolo.py`  
**역할**: YOLO 기반 얼굴 탐지 및 월드 좌표계 변환  
**주요 기능**: 얼굴 중심 픽셀 좌표 추출 → Hand-Eye Calibration 결과 행렬로 리맵핑 → `/remapped_coord` 토픽 발행  

**ROS 2 구성**:
- **노드명**: `face_yolo_node` (얼굴 탐지 및 좌표 변환)
- **토픽 발행**: `/remapped_coord` (3D 얼굴 위치 좌표)

---

### `object_detection.py`  
**역할**: YOLO 객체 인식 및 RealSense 깊이 데이터 결합 3D 위치 산출  
**주요 기능**: 객체 중심 좌표 및 Depth 정보를 사용해 월드 3차원 위치 계산 → `/get_3d_position` 서비스 서버 제공  

**ROS 2 구성**:
- **노드명**: `object_detection_node` (객체 인식 및 위치 산출)
- **서비스 서버**: `/get_3d_position` (3D 위치 응답 서비스)

---

### `get_keyword.py`  
**역할**: STT + LangChain 기반 음성 명령 키워드 추출  
**주요 기능**: Wake-up Word 인식 후 STT 수행 → 자연어 명령에서 핵심 키워드 추출 → `/get_keyword` 서비스 서버로 제공  

**ROS 2 구성**:
- **노드명**: `keyword_extraction_node` (음성 키워드 추출)
- **서비스 서버**: `/get_keyword` (키워드 반환 서비스)

---

### `robot_control.py`  
**역할**: 키워드 기반 로봇 동작 실행 및 위치 정보 활용 정밀 제어  
**주요 기능**:  
- `/get_3d_position` 서비스 클라이언트 호출로 물체 위치 획득  
- `/remapped_coord` 토픽 구독으로 얼굴 위치 실시간 수신  
- 그리퍼 제어 및 외력 감지 기반 예외처리 포함  

**ROS 2 구성**:
- **노드명**: `robot_control_node` (로봇 제어 및 상태 관리)
- **토픽 구독**: `/remapped_coord` (얼굴 위치)
- **서비스 클라이언트**: `/get_3d_position` (객체 위치 요청)

---

## 5. 도전 과제와 문제 해결 방법

| 문제 | 해결 방안 |
|------|------------|
| STT 상시 대기 시 자원 소모 큼 | Whisper 기반 wake-up 구조로 전환 |
| C270 카메라 좌표 오차 | 해상도 및 거리 실측 기반 리맵핑 |
| 캘리브레이션 오차 | 반복 실험 및 Offset 보정 |
| 로봇의 안전 경로 설정 어려움 | 충돌 제한 범위 선제 정의 및 필터 적용 |

---

## 6. 협업 내용 및 진행 과정

- 음성 인식, 키워드 추출 파트 개발 및 시스템 트리거 흐름 설계 주도
- 위치 인식 및 로봇 제어 파트 팀원과 데이터 흐름 협의 및 ROS 통신 연동
- 기술적 격차 해소를 위해 문서화 및 팀원 교육 진행

---

## 7. 성과 및 결과물

- ROS 2 기반 실시간 STT-기반 협동 로봇 제어 시스템 구축
- 사용자 얼굴 인식 → 물체 전달 → 음성 피드백까지 통합 동작
- Hand-Eye, 센서 기반 좌표 변환까지 포함한 전체 파이프라인 구현

---

## 8. 프로젝트 결과

- **3가지 핵심 기능 전부 구현 성공**
- **데모 시연 완수**: 사용자에게 정확히 물체 전달 성공
- **실환경 서비스 로봇 프로토타입 완성**
- **실제 협동 로봇 + 센서 + 인공지능 통합 경험 확보**

---

## 9. 기능 데모

🎥 [▶️ 시연 영상 보기](https://youtu.be/your_video_link)  
📷 _시스템 구조 다이어그램, YOLO 인식 화면 등 이미지 삽입 권장_

---

## 10. 후기 및 향후 개선 사항

- **음성 인식 한글 정확도** 향상 필요  
- **복잡한 환경에서의 좌표 안정성** 추가 보정 고려  
- **명령어 다양화 및 감정 피드백 시스템 확장 가능성 확인**

---

## 11. 개인적 성찰 및 배운 점

- 하드웨어 기반 시스템에서는 센서 신뢰성과 예외처리가 핵심임을 체감  
- 협업을 위한 소통 및 문서화의 중요성 인식  
- 단일 기능이 아닌 **전체 흐름을 고려한 설계 사고** 습득

---

## 12. 개선 및 확장 아이디어

- **AMR 기반 모바일 매니퓰레이터 확장**  
- **복지 시설과 실증 협력**  
- **표정 인식 등 정서적 반응 기능 추가**

